1. Create topic a topic having 3 brokers as --bootstrap-server. 
The brokers' ports are taken from the docker-compose_kafka.yml
The topic should have 3 partitions and replication-factor 3
The name of the topic is events1. 

/usr/bin/kafka-topics --create --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --replication-factor 3 --partitions 4 --topic events1
    
Create a second topic having 3 brokers as --bootstrap-server. 
The brokers' ports are taken from the docker-compose_kafka.yml
The topic should have 4 partitions and replication-factor 2
The name of the topic is events2.

/usr/bin/kafka-topics --create --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --replication-factor 2 --partitions 3 --topic events2

2. List all topics 

/usr/bin/kafka-topics --list --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094

3. Send data. Create a Producer and send data to events2 topic, using key and ',' as separator.

/usr/bin/kafka-console-producer --broker-list kafka:19092,kafka2:19093,kafka3:19094 --topic events2 --property parse.key="true" --property key.separator=","

3. Read the data. Create a Consumer and read data from events2 topic.  
No data will be shown because we do not have --from-beginning property

docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --topic events2 --property print.key="true" --property key.separator=","

4. Run consumer and print the partition
Check that messages with the same key go to the same partition. Notice, that messages may come in a different order, when they are in different partitions.

docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --topic events2 --property print.partition="true" --property print.key="true" --property key.separator=","

5. Run with specifying consumer group and printing the partition and the offset
Check that messages with the same key go to the same partition. Notice, that messages may come in a different order, when they are in different partitions.

docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --topic events2 --group my-group --property print.partition="true" --property print.offset="true" --property print.key="true" --property key.separator=","

6. Create a group of 2 consumers. 
Now we have two consumers within one group.

docker exec -ti kafka /usr/bin/kafka-console-consumer --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --topic events2 --group my-group --property print.partition="true" --property print.offset="true" --property print.key="true" --property key.separator=","

7. Send more data with our producer
Check how the messages are distributed between the 2 consumers. 

/usr/bin/kafka-console-producer --broker-list kafka:19092,kafka2:19093,kafka3:19094 --topic events2 --property parse.key="true" --property key.separator=","

8. Delete topic

/usr/bin/kafka-topics --bootstrap-server kkafka:19092,kafka2:19093,kafka3:19094 --delete --topic events2
/usr/bin/kafka-topics --bootstrap-server kafka:19092,kafka2:19093,kafka3:19094 --delete --topic events1


